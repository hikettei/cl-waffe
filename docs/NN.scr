
@begin(section)
@title(Neural Networks)

@begin(section)
@title(model-list)
Holds submodules in a list.

Model-List it contains are properly tracked by @cl:param(find-variables).

Note: This Layer is exported from Package @cl:param(cl-waffe).
@begin(section)
@title(Parameters)
@begin[lang=lisp](code)
(model-list list)
@end[lang=lisp](code)
@begin(deflist)

@def(list (list))
@term(an list of models)

@end(deflist)

This model can also be created by @cl:param(mlist)

@begin[lang=lisp](code)
(mlist models) ; -> [Model: MODEL-LIST]
@end[lang=lisp](code)
@end(section)

@begin(section)
@title(Forward)
@begin[lang=lisp](code)
(call (Model-List) index &rest args)
@end[lang=lisp](code)

Note that @cl:param(index) must be waffetensor.

To avoid this, @cl:param(mth) is available.

@begin[lang=lisp](code)
(call (mth 0 (Model-List)) &rest args)
@end[lang=lisp](code)

@begin(deflist)

@def(index (waffetensor of which data is fixnum))
@term(an index of models)

@def(args (list))
@term(arguments for index-th model)

@end(deflist)
@end(section)

@begin(section)
@title(Example)
@begin[lang=lisp](code)
(setq models (Model-List (list (linearlayer 10 1) (linearlayer 10 1))))
(call models (const 0) (!randn `(10 10)))
(call (mth 0 models) (!randn `(10 10)))
@end[lang=lisp](code)
@end(section)
@end(section)

@begin(section)
@title(Linearlayer)
Applies a linear transformation to the incoming data: @c((setq y (!add (!matmul x weight) bias)))

@begin(section)
@title(Parameters)
@begin[lang=lisp](code)
(LinearLayer in-features out-features &optional (bias T))
@end[lang=lisp](code)
@begin(deflist)

@def(in-features (fixnum))
@term(size of each input sample)
@def(out-features (fixnum))
@term(size of each output sample)
@def(bias (boolean))
@term(If set to nil, the layer will not learn an additive bias. default:t)

@end(deflist)
@end(section)

@begin(section)
@title(Shape)
@b(LinearLayer: (batch-size in-features) -> (batch-size out-features))

@begin(deflist)

@def(Input)
@term(x (Tensor) where the x is the shape of (batch-size in-features))
@def(Output)
@term(Output: an tensor that applied linearlayer, where the tensor is the shape of (batch-size out-features))

@end(deflist)
@end(section)

@begin(section)
@title(Forward)
@begin[lang=lisp](code)
(call (LinearLayer 10 1) x)
@end[lang=lisp](code)

@begin(deflist)

@def(x)
@term(the input tensor)

@end(deflist)
@end(section)

@begin(section)
@title(Example)
@begin[lang=lisp](code)
(call (LinearLayer 10 1) (!randn `(10 10)))
@end[lang=lisp](code)
@end(section)
@end(section)

@begin(section)
@title(DenseLayer)
Calling LinearLayer, and activation specified in @cl:param(activation).

@begin(section)
@title(Parameters)
@begin[lang=lisp](code)
(DenseLayer in-features out-features &optional (bias t) (activation :relu))
@end[lang=lisp](code)
@begin(deflist)

@def(in-features (fixnum))
@term(size of each input sample)

@def(out-features (fixnum))
@term(size of each output sample)

@def(bias (boolean))
@term(If set to nil, the layer will not learn an additive bias. default:t)

@def(activation (keyword or function))
@term(activation are following: :relu :sigmoid :tanh, If set to function, that is called as an activation.)

@end(deflist)
@end(section)

@begin(section)
@title(Shape)

@b(DenseLayer: (batch-size in-features) -> (batch-size out-features))

@begin(deflist)

@def(Input)
@term(x (Tensor) where the x is the shape of (batch-size in-features))
@def(Output)
@term(Output: an tensor that applied denselayer, where the tensor is the shape of (batch-size out-features))

@end(deflist)
@end(section)

@begin(section)
@title(Forward)
@begin[lang=lisp](code)
(call (DenseLayer 10 1) x)
@end[lang=lisp](code)

@begin(deflist)

@def(x)
@term(the input tensor)

@end(deflist)
@end(section)

@begin(section)
@title(Example)
@begin[lang=lisp](code)
(call (DenseLayer 10 1)  (!randn `(10 10)))
@end[lang=lisp](code)
@end(section)
@end(section)


@begin(section)
@title(Dropout)
When *no-grad* is nil, dropout randomly zeroes some elements of the given tensor with sampling bernoulli tensor of @cl:param(dropout-rate).

Futhermore, the outputs are scaled by (/ (- 1 (self dropout-rate))), (i.e.: This is a Inverted Dropout.). This means when *no-grad* is t (i.e.: during predicting) dropout simply returns the given tensor.

@begin(section)
@title(Parameters)
@begin[lang=lisp](code)
(dropout &optional (dropout-rate 0.5))
@end[lang=lisp](code)

@begin(deflist)
@def(dropout-rate)
@term(Dropout samples bernoulli distribution based on dropout-rate.)
@end(deflist)
@end(section)

@begin(section)
@title(Shape)

@b(Dropout: (Any) -> (The same as a input))

@begin(deflist)
@def(Input)
@term(Any is OK)
@def(Output)
@term(The same as given input's shape.)
@end(deflist)
@end(section)

@begin(section)
@title(Forward)
@begin[lang=lisp](code)
(setq x (!randn `(10 10)))
;#Const(((-0.59... -0.09... ~ 0.289... 0.390...)        
;                 ...
;        (1.447... 1.032... ~ -0.66... -0.55...)) :mgl t :shape (10 10))
(call (Dropout 0.5) x)
;#Const(((0.0 -0.19... ~ 0.0 0.0)        
;                 ...
;        (2.895... 2.064... ~ 0.0 -1.10...)) :mgl t :shape (10 10))
@end[lang=lisp](code)
@end(section)
@end(section)

@begin(section)
@title(BatchNorm2d)

Applies BatchNorm2D.

@begin(section)
@title(Parameters)
@begin[lang=lisp](code)
(BatchNorm2D in-features &key (affine t) (epsilon 1.0e-7))
@end[lang=lisp](code)

@begin(deflist)
@def(in-features)
@term(an excepted input of size)
@def(affine)
@term(if t, the model has trainable affine layers.)
@def(epsilon)
@term(the value used to the denominator for numerical stability. Default: 1.0e-7)
@end(deflist)
@end(section)

@begin(section)
@title(Shape)
@begin[lang=lisp](code)
(call (BatchNorm2D) x)
@end[lang=lisp](code)

@b(BatchNorm2D : (any, in-feature) -> (the same as input of shape))

@end(section)

@begin(section)
@title(Example)
@begin[lang=lisp](code)
(setq model (BatchNorm2D 10))
(call model (!randn `(30 10)))
@end[lang=lisp](code)

@end(section)
@end(section)

@begin(section)
@title(LayerNorm)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@begin(section)
@title(Embedding)
A simple lookup table object to store embedding vectors for NLP Models.

@begin(section)
@title(Parameter)
@begin[lang=lisp](code)
(Embedding vocab-size embedding-ize &key pad-idx)
@end[lang=lisp](code)
@begin(deflist)
@def(vocab-size)
@term((fixnum) size of the dictionary of embeddings)
@def(embedding-size)
@term((fixnum) the size of each embedding tensor)
@def(pad-idx)
@term(If specified, the entries at padding_idx do not contribute to the gradient. If nil, ignored.)
@end(deflist)
@end(section)

@begin(section)
@title(Shape)
@begin[lang=lisp](code)
(call (Embedding 10 10) x)
@end[lang=lisp](code)
@b(Embedding: (batch-size sentence-length) -> (batch-size sentence-len embedding-dim))

@begin(deflist)
@def(x)
@term(input x, where each element are single-float (like 1.0, 2.0 ...))
@end(deflist)
@end(section)

@begin(section)
@title(Example)
@begin[lang=lisp](code)
(setq model (cl-waffe.nn:Embedding 10 20))

(call model (!ones `(1 10)))
#Const((((-0.01... -0.01... ~ 0.013... 0.002...)         
                   ...
         (-0.01... -0.01... ~ 0.013... 0.002...))) :mgl t :shape (1 10 20))
@end[lang=lisp](code)
@end(section)
@end(section)


@begin(section)
@title(RNN)
Applies a multi-layer RNN with tanh or ReLU.

The assumption is that (setf !aref)'s backward contributes to it.

@begin(section)
@title(Parameters)
@begin[lang=lisp](code)
(RNN input-size hidden-size &key (num-layers 1) (activation :tanh) (bias t) (dropout nil) (biredical nil))
@end[lang=lisp](code)

@begin(deflist)
@def(input-size)
@term(The number of excepted features of x)

@def(hidden-size)
@term(The number of features in hidden-layer)

@def(num-layers)
@term(Number of reccurent layers)

@def(activation)
@term(Can be either :tanh or :relu)

@def(bias)
@term((boolean) If t, the model has a trainable bias.)

@def(dropout)
@term((boolean) If t, the model has a dropout layer.)

@def(biredical)
@term((boolean) If t, the model become a biredical RNN)
@end(deflist)
@end(section)

@begin(section)
@title(Shape)
@begin[lang=lisp](code)
(call (RNN 10 10) x &optional (hs nil))
@end[lang=lisp](code)
@b(RNN : (batch-size sentence-length input-size) -> (batch-size sentence-length hidden-size))

@begin(deflist)
@def(x)
@term(the input x where the shape is (batch-size sentence-length input-size))
@def(hs)
@term(The last hidden-state. if nil, the model creates a new one.)
@end(deflist)
@end(section)

@begin(section)
@title(Example)
@begin[lang=lisp](code)
(setq model (RNN 10 20))
(setq embedding (Embedding 10 10))
(call model
  (call embedding (!one `(10 10))))

;#Const((((-1.46... -1.46... ~ -5.53... 1.766...)         
;                   ...
;         (-1.46... -1.46... ~ -5.53... 1.766...))        
;                 ...
;        ((-1.46... -1.46... ~ -5.53... 1.766...)         
;                   ...
;         (-1.46... -1.46... ~ -5.53... 1.766...))) :mgl t :shape (10 10 20))
  
@end[lang=lisp](code)
@end(section)


@end(section)

@begin(section)
@title(LSTM)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@begin(section)
@title(GRU)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@begin(section)
@title(MaxPooling)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@begin(section)
@title(AvgPooling)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@begin(section)
@title(Conv1D)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@begin(section)
@title(Conv2D)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@begin(section)
@title(Transformer)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@begin(section)
@title(TransformerEncoderLayer)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@begin(section)
@title(TransformerDecoderLayer)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@begin(section)

@title(CrossEntropy)
@cl:with-package[name="cl-waffe.nn"](
@cl:doc(function cross-entropy)
)
@end(section)

@begin(section)
@title(SoftMaxCrossEntropy)
@cl:with-package[name="cl-waffe.nn"](
@cl:doc(function softmax-cross-entropy)
)
@end(section)

@begin(section)
@title(MSE)
@cl:with-package[name="cl-waffe.nn"](
@cl:doc(function mse)
)
@end(section)

@begin(section)
@title(L1Norm)
@cl:with-package[name="cl-waffe.nn"](

)
@end(section)

@begin(section)
@title(L2Norm)
@cl:with-package[name="cl-waffe.nn"](

)
@end(section)

@begin(section)
@title(BinaryCrossEntropy)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@begin(section)
@title(KLdivLoss)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@begin(section)
@title(CosineSimilarity)
@cl:with-package[name="cl-waffe.nn"](
)
@end(section)

@end(section)